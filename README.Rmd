---
output: github_document

# Via https://bookdown.org/yihui/bookdown/citations.html
biblio-style: "apalike"
bibliography: references.bib 
link-citations: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

# Feature Rank: ensemble feature ranking for variable selection

Ensemble feature ranking for variable selection in SuperLearner ensembles [@polley2021package], based on @effrosynidis2021evaluation. Multiple algorithms estimate a ranking of the strength of the relationship between predictors and the outcome in the training set, and these rankings are combined into a single ranking via an aggregation method (reciprocal ranking currently). The final ranking can then be cut at a certain number of variables (e.g. top 10 predictors, top 70%, etc.) to create one or more feature selection wrappers for SuperLearner. The result should generally be more robust and stable than feature selection using a single algorithm.

## Install

```{r install, eval = FALSE}
# install.packages("remotes")
remotes::install_github("ck37/featurerank")
```

## Algorithms

Currently implemented algorithms are:

  * Feature ranking: correlation, glm, glmnet, random forest, bart, xgboost + shap, variance
  * Rank aggregation: reciprocal ranking

## Example

A minimal example to demonstrate how the package can be used.

```{r setup, include=FALSE}
# We include library() here so that the output is suppressed, and again
# later in the demo just so people can see it.
library(SuperLearner)
library(glmnet)
library(embarcadero)
library(dbarts)
```

### Prepare dataset

```{r prep_dataset}
# TODO: switch to a less problematic demo dataset.
data(Boston, package = "MASS")

# Use "chas" as our outcome variable, which is binary.
x = subset(Boston, select = -chas)
y = Boston$chas
family = binomial()

```

### Create feature ranking library

Specify the feature ranking wrappers for the ensemble library.

```{r create_library}
library(featurerank)

# Modify RF feature ranker to use 100 trees.
featrank_randomForest100 =
  function(...) featrank_randomForest(ntree = 100L, ...)

# Specify the set of feature ranking algorithms.
ensemble_rank_custom =
  function(top_vars, ...)
    ensemble_rank(fn_rank = c(featrank_cor, featrank_randomForest100,
                              featrank_glm, featrank_glmnet,
                              #featrank_shap, # too verbose currently
                              featrank_dbarts),
                  top_vars = top_vars,
                  ...)

# There are 13 total vars so try dropping 1 of them.
top12 = function(...) ensemble_rank_custom(top_vars = 12, ...)

# Try dropping worst 2 predictors.
top11 = function(...) ensemble_rank_custom(top_vars = 11, ...)
```

### Use in SuperLearner

```{r use_sl}
library(SuperLearner)

# Seems to work correctly.
set.seed(1)
sl = SuperLearner(y, x, family = binomial(),
                  cvControl = list(V = 10L, stratifyCV = TRUE),
                  SL.library =
                    list("SL.mean",
                         # Try two ensemble screening options vs. all predictors.
                         c("SL.glm", "top12", "top11", "All")))


# We do achieve an AUC benefit.
ck37r::auc_table(sl, y = y)[, -6]

# Which features were dropped (will show FALSE below)?
t(sl$whichScreen)

```

### Assess ranking stability

```{r stability}
# Check if we see stability across multiple runs,
# especially for comparison to individual feature ranking algorithms.
# (See stability scores in Table 3 of paper.)
set.seed(2)
results =
  do.call(rbind.data.frame,
          lapply(1:10,
                 function(i) top12(y, x, family,
                                   return_ranking = TRUE)))
names(results) = names(x)
# Stability looks excellent.
results
```

## References

<div id="refs"></div>

