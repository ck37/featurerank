---
title: "Ensemble ranking test"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# remotes::install_github("ck37/ck37r")
library(ck37r)
library(SuperLearner)

ck37r::load_all_code("R", verbose = TRUE)
```

## Data prep

```{r tests}

# TODO: switch to a less problematic demo dataset.
data(Boston, package = "MASS")

set.seed(1, "L'Ecuyer-CMRG")

# Subset rows to speed up example computation.
row_subset = sample(nrow(Boston), 200)

Boston = Boston[row_subset, ]

# Use "chas" as our outcome variable, which is binary.
x = subset(Boston, select = -chas)
y = Boston$chas
family = binomial()

```

## Feature ranking wrappers

Each wrapper should return a ranking of all variables that are being analyzed. They should not do any subsetting.

TODO

 * look at Sara Moore's [feature selection repo](https://github.com/saraemoore/SLScreenExtra) for more ideas.
 * Also Ron's email about additional feature selection algorithms to try, e.g. from the main paper
 * Use tidymodels or mlr3 - do they have good feature selection/ranking options?

```{r fn_feature_ranking}
# Univariate correlation 
(res1 = featrank_cor(y, x, family))

# random forest
# These aren't named currently.
(res2 = featrank_randomForest(y, x, family, ntree = 10))


# TODO: ranger - started in R/ folder.
# TODO: glm (skip? might require a ridge penalty)
# TODO: glmnet (support ridge, lasso, and elastic net)
# TODO: others 
```

### Ranking dataframe

```{r integrate_rankings}
# Create a dataframe containing all rankings.
rank_df = data.frame(cor = res1, rf = res2)
rank_df
```

## Aggregation wrappers

This should be one or more was of aggregating multiple feature ranking algorithms into an overall ranking.

```{r fn_aggregation}
# TODO: reciprocal ranking
agg_reciprocal_rank = function(rank_df) {
  # For each feature, sum the inverse of its ranks, then invert the sum.
  ranks = apply(rank_df, MARGIN = 1, function(row) {
     1 / sum(row^-1)
  })
  return(rank(ranks))
}

# Give it a ranking of multiple algorithms, and it will return the overall ranking.
agg_reciprocal_rank(rank_df)
```

## Overall wrapper

This wrapper should be usable as a feature selection wrapper in SuperLearner, so
we would want to specify some number of variables to select based on the final
ranking.

```{r wrapper_overall}
# Specify the data, feature ranking wrappers, aggregation wrapper, and the number
# of features to select.

ensemble_rank =
  function(Y, X, family,
           # Feature ranking algorithms.
           fn_rank = c(featrank_cor, featrank_randomForest),
           # Rank aggregation algorithm.
           fn_agg = agg_reciprocal_rank,
           # How many variables we want to select. Top 50% by default.
           top_vars = ceiling(ncol(X) / 2),
           # Not used.
           ...)  {
    
    # Run each feature ranking algorithm.
    rank_df = do.call(cbind.data.frame,
                      lapply(fn_rank, function(fn) fn(Y, X, family)))
    
    # Calculate the aggregate ranking.
    ranking = fn_agg(rank_df)
    
    # Apply the ranking to select the top X variables.
    whichVariable <- (ranking <= top_vars)
    return(whichVariable)
  }

# Create a custom RF ranker that only uses 10 trees, to speed up testing.
featrank_randomForest10 =
  function(...) featrank_randomForest(ntree = 10, ...)


featrank_randomForest100 =
  function(...) featrank_randomForest(ntree = 100, ...)

debugonce(ensemble_rank)
(result = ensemble_rank(y, x, family, fn_rank = c(featrank_cor, featrank_randomForest10)))

# Create a custom ensemble rank feature selector, using the RF learner.
# Also customizing top_vars if desired.
ensemble_rank_custom =
  function(...) ensemble_rank(fn_rank = c(featrank_cor,
                                          featrank_randomForest100),
                              # There are 13 total vars so try dropping 1 of them.
                              top_vars = 12,
                              ...)
```

## Test SuperLearner

```{r test_sl}
# Seems to work correctly.
sl = SuperLearner(y, x, family = binomial(),
                  cvControl = list(V = 10L, stratifyCV = TRUE),
                  SL.library =
                    list("SL.mean",
                         # Try two screening options: ensemble_rank_custom or All.
                         c("SL.glm", "ensemble_rank_custom", "All")))

# No AUC benefit, but maybe we need better/more feature rankings. Or this dataset just
# doesn't have many noise features.
# Plus we aren't used any complex ML algorithms like xgboost / BART / RF.
ck37r::auc_table(sl, y = y)

# Which feature was dropped in the final SL?
names(x)[!sl$whichScreen["ensemble_rank_custom", ]]
# Full results:
sl$whichScreen
```

