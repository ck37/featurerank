---
title: "Ensemble ranking test"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# remotes::install_github("ck37/ck37r")
library(ck37r)
library(SuperLearner)

ck37r::load_all_code("R", verbose = TRUE)
```

General to-dos:

* Support observation weights.

## Data prep

```{r tests}

# TODO: switch to a less problematic demo dataset.
data(Boston, package = "MASS")

# Skip row subsetttings

#set.seed(1, "L'Ecuyer-CMRG")

# Subset rows to speed up example computation.
#row_subset = sample(nrow(Boston), 200)

#Boston = Boston[row_subset, ]

# Use "chas" as our outcome variable, which is binary.
x = subset(Boston, select = -chas)
y = Boston$chas
family = binomial()

```

## Feature ranking wrappers

Each wrapper should return a ranking of all variables that are being analyzed. They should not do any subsetting.

TODO

 * look at Sara Moore's [feature selection repo](https://github.com/saraemoore/SLScreenExtra) for more ideas.
 * Also Ron's email about additional feature selection algorithms to try, e.g. from the main paper
 * Use tidymodels or mlr3 - do they have good feature selection/ranking options?

```{r fn_feature_ranking}
# Univariate correlation 
(res1 = featrank_cor(y, x, family))

# rad does have the high p-value and medv has the lowest.
sapply(x, function(x) {
        cor.test(x, y = y, method = "pearson")$p.value
})

# random forest
# These aren't named currently.
(res2 = featrank_randomForest(y, x, family, ntree = 10))


(res3 = featrank_glm(y, x, family))

# Compare to glm() manual results.
reg = glm(y ~ ., data = x, family = binomial())
summary(reg)

# TODO: ranger - started in R/ folder.
# TODO: glmnet (support ridge, lasso, and elastic net)
# TODO: others 
```

### Ranking dataframe

```{r integrate_rankings}
# Create a dataframe containing all rankings.
# Interestingly "rad" and "tax" have such divergent results for glm.
(rank_df = data.frame(cor = res1, rf = res2, glm = res3))
```

## Aggregation wrappers

This should be one or more was of aggregating multiple feature ranking algorithms into an overall ranking.

### Reciprocal rank

```{r fn_aggregation}
# TODO: reciprocal ranking
agg_reciprocal_rank = function(rank_df) {
  # For each feature, sum the inverse of its ranks, then invert the sum.
  ranks = apply(rank_df, MARGIN = 1, function(row) {
     1 / sum(row^-1)
  })
  return(rank(ranks))
}

# Give it a ranking of multiple algorithms, and it will return the overall ranking.
agg_reciprocal_rank(rank_df)
```

## Overall wrapper

This wrapper should be usable as a feature selection wrapper in SuperLearner, so
we would want to specify some number of variables to select based on the final
ranking.

```{r wrapper_overall}
# Specify the data, feature ranking wrappers, aggregation wrapper, and the number
# of features to select.

ensemble_rank =
  function(Y, X, family,
           # Feature ranking algorithms.
           fn_rank = c(featrank_cor, featrank_randomForest, featrank_glm),
           # Rank aggregation algorithm.
           fn_agg = agg_reciprocal_rank,
           # How many variables we want to select. Top 50% by default.
           top_vars = ceiling(ncol(X) / 2),
           # Set as TRUE for debugging purposes
           return_ranking = FALSE,
           # Not used.
           ...)  {
    
    # Run each feature ranking algorithm.
    rank_df = do.call(cbind.data.frame,
                      lapply(fn_rank, function(fn) fn(Y, X, family)))
    
    # Calculate the aggregate ranking.
    ranking = fn_agg(rank_df)
    
    if (return_ranking) {
      return(ranking)
    }
    
    # Apply the ranking to select the top X variables.
    whichVariable <- (ranking <= top_vars)
    return(whichVariable)
  }

# Create a custom RF ranker that only uses 10 trees, to speed up testing.
featrank_randomForest10 =
  function(...) featrank_randomForest(ntree = 10, ...)


featrank_randomForest100 =
  function(...) featrank_randomForest(ntree = 100, ...)

(result = ensemble_rank(y, x, family,
                        fn_rank = c(featrank_cor, featrank_randomForest10, featrank_glm)))

# Create a custom ensemble rank feature selector, using the RF learner.
# Also customizing top_vars to drop a single feature.
ensemble_rank_custom =
  function(...) ensemble_rank(fn_rank = c(featrank_cor,
                                          featrank_randomForest100,
                                          featrank_glm),
                              # There are 13 total vars so try dropping 1 of them.
                              top_vars = 12,
                              ...)
```

### Stability assessment

```{r stability}
# Check if we see stability across multiple runs,
# especially for comparison to individual feature ranking algorithms.
# (See stability scores in Table 3 of paper.)
results = do.call(rbind.data.frame, lapply(1:10, function(i) ensemble_rank_custom(y, x, family, return_ranking = TRUE)))
names(results) = names(x)
# Stability looks excellent.
results
```

## Test SuperLearner

```{r test_sl}
# Seems to work correctly.
set.seed(1)
sl = SuperLearner(y, x, family = binomial(),
                  cvControl = list(V = 10L, stratifyCV = TRUE),
                  SL.library =
                    list("SL.mean",
                         # Try two screening options: ensemble_rank_custom or All.
                         c("SL.glm", "ensemble_rank_custom", "All")))


# We do achieve a small AUC benefit - schweet.
ck37r::auc_table(sl, y = y)

# Which feature was dropped in the final SL?
names(x)[!sl$whichScreen["ensemble_rank_custom", ]]
# Full results:
sl$whichScreen
```

